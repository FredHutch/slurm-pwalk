# slurm functions and commands

# commands
sbatch_cmd="sbatch"

# sbatch job count limit
#sbatch_simultaneous_tasks=15
#sbatch_simultaneous_tasks=5

# sbatch job array size limit - run multiple arrays if folder over this limit
#slurm_maxarraysize=$(scontrol show config | awk '/^MaxArraySize/ {print $3}')
export CRAWL_JOB_ARRAY_MAX_SIZE=1000

# sbatch command line arguments
#sbatch_partition="boneyard"
#sbatch_mail_type="FAIL"
#sbatch_mail_user="bmcgough"
#sbatch_time="1-0"   # days-hours
# pwalk parallelizes, so we run one job per folder
#sbatch_cpus_per_task="3"
# the crawl tag will be appended to this to become job-name
#sbatch_job_name_prefix="storcrawl_"

# schedule the crawl jobs
function slurm_run_crawl_jobs {
  sbatch_job_name="${sbatch_job_name_prefix}${CRAWL_TAG}"
  # chunk into array sets at array size limit
  num_job_arrays=$((${CRAWL_JOB_ARRAY_SIZE}/${CRAWL_JOB_ARRAY_MAX_SIZE}+1))
  # set per-array job limit
  array_simultaneous_tasks=$((${sbatch_simultaneous_tasks}/${num_job_arrays}))
  # loop through job arrays
  for i in $(seq 1 $num_job_arrays)
  do
    # set our end numbers for the array
    if [ "$i" -eq "$num_job_arrays" ]
    then
      end_id=$((${CRAWL_JOB_ARRAY_SIZE}%${sbatch_maxarraysize}))
    else
      end_id="${sbatch_maxarraysize}"
    fi

    # build the command line
    cmd="--array=1-${end_id}%${array_simultaneous_tasks}"
    if [ -n "${sbatch_mem}" ]
    then
      cmd="$cmd --mem=$sbatch_mem"
    fi
    cmd="$cmd --partition=$sbatch_partition"
    cmd="$cmd --mail-type=$sbatch_mail_type"
    cmd="$cmd --mail-user=$sbatch_mail_user"
    cmd="$cmd --time=$sbatch_time"
    cmd="$cmd --cpus-per-task=$sbatch_cpus_per_task"
    cmd="$cmd --job-name=$sbatch_job_name"
    cmd="$cmd --output=${CRAWL_OUTPUT_DIR}/output_%A_%a.stdout"
    cmd="$cmd --error=${CRAWL_OUTPUT_DIR}/output_%A_%a.stderr"
    cmd="$cmd --wrap=\"$0 --action crawl\""

    # run it
    sbatch_output=$(eval $sbatch_cmd $cmd)

    # check it
    if [ $? -eq 0 ]
    then
      echo "crawl array job launched: $sbatch_output"
      storcrawl_log "crawl array job launched: $sbatch_output"
    else
      error_exit "Ooops, unable to launch crawl jobs, exiting..."
    fi
  done
}

# schedule the monitor job
function slurm_run_monitor_job {
  # build the command line
  cmd="--partition=$sbatch_partition" 
  if [ -n "${sbatch_mem}" ]
  then
    cmd="$cmd --mem=$sbatch_mem"
  fi
  cmd="$cmd --mail-type=$sbatch_mail_type"
  cmd="$cmd --mail-user=$sbatch_mail_user"
  cmd="$cmd --time=$sbatch_time"
  cmd="$cmd --output=${CRAWL_OUTPUT_DIR}/output_monitor_%a.%J.stdout"
  cmd="$cmd --error=${CRAWL_OUTPUT_DIR}/output_monitor_%a.%J.stderr"
  cmd="$cmd --cpus-per-task=1"
  cmd="$cmd --job-name=${sbatch_job_name}_monitor"
  cmd="$cmd --wrap=\"$0 --action monitor\""

  # run it
  sbatch_output=$(eval $sbatch_cmd $cmd)

  # check it
  if [ $? -eq 0 ]
  then
    echo "crawl monitor job launched: $sbatch_output"
    storcrawl_log "crawl monitor job launched: $sbatch_output"
  else
    error_exit "Ooops, unable to launch monitor job, exiting..."
  fi
}

# schedule the requeue job
function slurm_run_requeue_job {
  sbatch_job_name="${sbatch_job_name_prefix}${CRAWL_TAG}"
  # build the command line
  cmd="--partition=$sbatch_partition" 
  if [ -n "${sbatch_mem}" ]
  then
    cmd="$cmd --mem=$sbatch_mem"
  fi
  cmd="$cmd --mail-type=$sbatch_mail_type"
  cmd="$cmd --mail-user=$sbatch_mail_user"
  cmd="$cmd --time=$sbatch_time"
  cmd="$cmd --output=${CRAWL_OUTPUT_DIR}/output_requeue_%a.%J.stdout"
  cmd="$cmd --error=${CRAWL_OUTPUT_DIR}/output_requeue_%a.%J.stderr"
  cmd="$cmd --cpus-per-task=1"
  cmd="$cmd --job-name=$sbatch_job_name"
  cmd="$cmd --dependency=singleton"
  cmd="$cmd --wrap=\"$0 --action requeue\""

  # run it
  sbatch_output=$(eval $sbatch_cmd $cmd)

  # check it
  if [ $? -eq 0 ]
  then
    echo "crawl requeue job launched: $sbatch_output"
    storcrawl_log "crawl requeue job launched: $sbatch_output"
  else
    error_exit "Ooops, unable to launch requeue job, exiting..."
  fi
}

# schedule the cleanup job
function slurm_run_cleanup_job {
  sbatch_job_name="${sbatch_job_name_prefix}${CRAWL_TAG}"
  # build the command line
  cmd="--partition=$sbatch_partition" 
  if [ -n "${sbatch_mem}" ]
  then
    cmd="$cmd --mem=$sbatch_mem"
  fi
  cmd="$cmd --mail-type=$sbatch_mail_type"
  cmd="$cmd --mail-user=$sbatch_mail_user"
  cmd="$cmd --time=$sbatch_time"
  cmd="$cmd --output=${CRAWL_OUTPUT_DIR}/output_housekeeper_%a.%J.stdout"
  cmd="$cmd --error=${CRAWL_OUTPUT_DIR}/output_housekeeper_%a.%J.stderr"
  cmd="$cmd --cpus-per-task=$sbatch_cpus_per_task"
  cmd="$cmd --job-name=$sbatch_job_name"
  cmd="$cmd --dependency=singleton"
  cmd="$cmd --wrap=\"$0 --action cleanup\""

  # run it
  sbatch_output=$(eval $sbatch_cmd $cmd)

  # check it
  if [ $? -eq 0 ]
  then
    echo "crawl cleanup job launched: $sbatch_output"
    storcrawl_log "crawl cleanup job launched: $sbatch_output"
  else
    error_exit "Ooops, unable to launch cleanup job, exiting..."
  fi
}
