### storcrawl config
# this config file is sourced in bash, so the valid format is:
#   <key>=<value> (no spaces!)

# start paths
# storcrawl will distribute each folder under these paths for crawling by pwalk
# separate dirs with newline - spaces and other characters should be OK in folder name
# files _in_ these folders will not be crawled, sorry
start_paths="/fh/fast
/home"
#start_paths="/fh/fast/shou_w
#/home/shou
#/home/bmcgough"

# keep this number of crawl tables in the db (older ones will be dropped)
storcrawl_keep=5

# slurm-specific (TODO: move out and make modular)
# sbatch queue modulo
sbatch_simultaneous_tasks=15
#sbatch_simultaneous_tasks=5
#slurm_maxarrayjob=$(scontrol show config | awk '/^MaxArraySize/ {print $3}')
sbatch_maxarraysize=1000

sbatch_partition="boneyard"
sbatch_mail_type="BEGIN,END,FAIL"
sbatch_mail_user="bmcgough"
sbatch_time="1-0"   # days-hours
sbatch_cpus_per_task="12"
sbatch_job_name_prefix="storcrawl_"   # will be appended with the current TAG

### command locations
# database cli (psql for now)
db_cli="psql" 
# pwalk (https://github.com/fizwit/filesystem-reporting-tools)
pwalk_cmd="/app/bin/pwalk"
# scheduling command (sbatch/slurm for now)
sbatch_cmd="sbatch"
# csvquote (https://github.com/dbro/csvquote)
csvquote_cmd="/home/bmcgough/github/bmcgough/csvquote/csvquote"
# we use Lmod to load environment modules
lmod_init="/app/Lmod/lmod/lmod/init/bash"
modulefile_dir="/app/easybuild/modules/all"   
psql_module="PostgreSQL/9.5.5-foss-2016b"

### db config
# you will need to create ~/.pgpass with the appropriate credentials in it
db_host="mydb"
db_port=32048
database="storcrawldb"
db_user="storcrawl_user"

# ro user will be created and granted ro perms on DB
db_ro_user="storcrawl_ro"
db_ro_password="scicomp_rulz"

# psql standard command line flags
db_flags="-q "

# construct the connection string used for all psql commands
db_conn_str="-h $db_host -p $db_port -d $database -U $db_user $db_flags"

### scripts
# scripts run before and after the storcrawl run, and during cleanup
#   before - run by main script before pwalk tasks are scheduled
#   after - run by clean up task after all crawls are done
#   remove - run when a tag is removed (ex: remove table create before)
# any executable file in these direcctories will be run
# they have access to storcrawl env vars (see docs)
before_script_dir="before_scripts.d"
after_script_dir="after_scripts.d"
remove_script_dir="remove_scripts.d"
# owner script - takes path, returns owner
# called by each pwalk task with the name of the folder being walked
owner_script="./get_owner.sh"

### table config
# tables prefixes - you don't need to mess with these for the most part
source_tbl="filesystem_sources_"
file_stat_tbl="file_stat_"
file_meta_tbl="file_meta_"
file_tbl="file_metadata_"
folder_tbl="folders_"
log_tbl="log_"
report_tbl="report_"

# main view name (how you will query most recent result set)
storcrawldb_view="file_metadata"

### storcrawl function files
# functions file locations - you also don't need to mess with these
storcrawldb_functions="storcrawldb.functions"
storcrawldb_db_functions="storcrawldb.db_functions"

# storcrawl output
# exported to the environment as STORCRAWLDB_CSV_DIR
csv_dir="csv_"

# slurm output
# exported to the environment as STORCRAWLDB_OUTPUT_DIR
output_dir="output_"

