### storcrawl config
# this config file is sourced in bash, so the valid format is:
#   <key>=<value> (no spaces!)

# where are my bash functions?
bash_functions_file="storcrawldb.bash_functions"

# where are my database functions?
db_functions_file="storcrawldb.postgresql_functions"

# where are my scheduler functions?
scheduler_functions_file="storcrawldb.slurm_functions"

# keep this number of crawl results in the DB (older ones will be dropped)
storcrawl_keep=2

# log function to call for logging
storcrawl_log_func="db_storcrawl_log"   # see storcrawldb.postgresql_functions

# slurm-specific (TODO: move out and make modular)
# sbatch queue modulo
# - warning, this is per array right now, so with sim tasks = 5 and 5 arrays = 25
sbatch_simultaneous_tasks=3
#slurm_maxarrayjob=$(scontrol show config | awk '/^MaxArraySize/ {print $3}')
sbatch_maxarraysize=1000

sbatch_partition="boneyard"
sbatch_mail_type="BEGIN,END,FAIL"
sbatch_mail_user="bmcgough"
sbatch_time="1-0"   # days-hours
sbatch_cpus_per_task="12"
sbatch_job_name_prefix="storcrawl_"   # will be appended with the current TAG

### command locations
# database cli (psql for now)
db_cmd="psql" 

# pwalk (https://github.com/fizwit/filesystem-reporting-tools)
pwalk_cmd="/app/bin/pwalk"
# appended to pwalk_cmd
pwalk_opts="--NoSnap --maxthreads=32"

# scheduler command
# currently only used as a check
# see storcrawldb.scheduler_functions for slurm implementation)
scheduler_cmd="sbatch"
queue_cmd="squeue"

# monitor command
# home grown script to pull job array IDs from DB and check periodically
# intervals in seconds - check=look and exit if jobs done - log=write log of job array status
monitor_cmd="storcrawldb.monitor"
monitor_check_interval=300
monitor_log_interval=3600

# csvquote (https://github.com/dbro/csvquote)
csvquote_cmd="/app/bin/csvquote"

### scripts
# scripts run before and after the storcrawl run, and during cleanup
#   before - run by main script before pwalk tasks are scheduled
#   after - run by clean up task after all crawls are done
# any executable file in these direcctories will be run
# they have access to storcrawl env vars (see docs)
before_script_dir="before_scripts.d"
after_script_dir="after_scripts.d"
# owner script - takes path, returns owner
# called by each pwalk task with the name of the folder being walked
owner_script="./get_owner.sh"

### table config
# tables prefixes - you don't need to mess with these for the most part
source_tbl="filesystem_sources_"
file_tbl="file_metadata_"
folder_tbl="folders_"
log_tbl="log_"
report_tbl="report_"

# main view name (how you will query most recent result set)
storcrawldb_view="file_metadata"

# directory prefix for raw pwalk output
# is appended with tag
# exported to the environment as CRAWL_CSV_DIR
csv_dir="csv_"

# directory prefix for slurm output
# is appended with tag
# exported to the environment as CRAWL_OUTPUT_DIR
output_dir="output_"

# slack integration
slack_webhook_url="https://hooks.slack.com/services/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"
slack_channel="#storcrawl"

